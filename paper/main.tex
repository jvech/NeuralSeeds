\documentclass{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[style=numeric]{biblatex}
\addbibresource{refs.bib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}



\title{Seeds Germination Detection Using Convolutional SSD-like Architectures}
\author{Juan Valencia}

\begin{document}
\maketitle

\begin{abstract}
    In this article SSD-like models are implemented to identify seeds in
    germination using a methodology based on \cite{main:seedproject} work.
\end{abstract}

%\begin{IEEEkeywords}
%    Mean Average Precision (mAP), Bounding Box, Anchor Boxes, Non Maximum
%    Suppression
%\end{IEEEkeywords}

\section{Introduction}

\section{Materials and Methods}

\subsection{Dataset}
The dataset used was taken from the image acquisition process made by a project
with the same goal \cite{main:seedproject}. It consists of 3 folders which
contain the annotation of three seeds species (ZeaMays, SecaleCereale and
PennisetumGlaucum) during its germination, figure \ref{fig:zeamays_demo} shows
one example of the image with its annotations.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/theory/Dataset_example.png}
    \caption{ZeaMays Seeds with their respective ground truth bounding boxes}
    \label{fig:zeamays_demo}
\end{figure}
The images of the dataset are in jpeg format and its respective annotations are
xml files generated by \url{https://cvats.org}


\subsection{Multiple Objects Detection Problem}
Let $\{ I_n \in \mathbb{R}^{R \times C}, B_n \in \mathbb{R}^{M_n \times 4}, L_n
\in \{l_1, l_2\}^{M_n}\}^N_n$ be an 1 input - 2 outputs set holding $N$
labeled images, where $I_n$ is the $n$-th image with $R$ rows and $C$ columns.
$B_n$ and $L_n$ contains the bounding boxes and the classes of the $M_n$ objects
of interest from the image $I_n$ respectively.\par

\subsection{Model Architecture}
There are two approaches to perform object detection over multiple elements. The
first is based on two stage processing which is present in architectures like
R-CNN \cite{detectors:R-CNN} and Faster R-CNN \cite{detectors:Faster_R-CNN}
where is necessary to process the images twice to generate proposal regions an
then classify them, while in architectures like YOLOv3 \cite{detectors:yolov3}
or SSD \cite{detectors:SSD} the image is passed once through the network. Models
like R-CNN and Faster R-CNN tend to be more accurate than YOLO or SSD
architectures but are slower \cite{detectors:comparison}. Thus we decided to use
SSD framework due to its improvements in speed/accuracy made in architectures
like RetinaNet \cite{detectors:retinanet}.\par

SSD architectures are made of three parts (see the Figure
\ref{fig:SSD_architecture}):
\begin{itemize}
    \item Backbone
    \item Bottleneck
    \item Head
\end{itemize}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/theory/SSD_architecture.png}
    \caption{SSD Architecture}
    \label{fig:SSD_architecture}
\end{figure}
\par

The backbone is responsible of generate the feature maps which will be extracted
to the bottleneck, it consist of a set of convolutional layers $\{W_l \in
\mathbb{R}^{P_l \times P_l \times D_l}\}_{l=1}^L$ where $P_l$ denote the $l$-th
layer size and number of filters respectively. The backbone will generate three features maps
$\hat{Z}_1 \in \mathbb{R}^{R_{L_1} \times C_{L_1} \times D_{L_1}}$, $\hat{Z}_2
\in \mathbb{R}^{R_{L_2} \times C_{L_2} \times D_{L_2}}$ and $\hat{Z}_3 \in
\mathbb{R}^{R_{L_3} \times C_{L_3} \times D_{L_3}}$ as follows:
\begin{equation}
    \begin{array}{cc}
        \hat{Z}_1 &= (\varphi_{L_1} \circ \dots \circ \varphi_{1})(I) \\
        \hat{Z}_2 &= (\varphi_{L_2} \circ \dots \circ \varphi_{1})(I) \\ 
        \hat{Z}_3 &= (\varphi_{L_3} \circ \dots \circ \varphi_{1})(I) \\ 
    \end{array}
    \label{eq:Backbone}
\end{equation}
where $L_3 > L_2 > L_1$ correspond to the layer position in the backbone, $F_l$
= $\varphi_l(F_{l-1}) = \nu_l(W_l \otimes F_{l-1} + B_l) \in \mathbb{R}^{R_l
\times C_l \times D_l}$ is a tensor holding $D_l$ feature maps at the $l$-th
layer, $\varphi_l : \mathbb{R}^{R_{l-1} \times C_{l-1} \times D_{l-1}} \mapsto
\mathbb{R}^{R_{l} \times C_{l} \times D_{l}}$ is a function among the backbone
layers, $B_l \in \mathbb{R}^{R_{l} \times C_{l} \times D_{l}}$ is a bias tensor
and $\nu_l(\cdot)$ is a non-linear function like $ReLu(x) = \max(0, x)$.\par

The bottleneck mix the feature maps $Z_1, Z_2, Z_3$ from the backbone to share
the features at different heights of the backbone module, it generates another
three feature maps $Z_1, Z_2, Z_3$ with the same dimensions as $\hat{Z}_1,
\hat{Z}_2, \hat{Z}_3$ respectively:
\begin{equation}
    \begin{split}
        Z_1 &= (\varphi_{K_1} \circ \cdots \circ \varphi_1)(Z_1, Z_2, Z_3)\\
        Z_2 &= (\varphi_{K_2} \circ \cdots \circ \varphi_1)(Z_1, Z_2, Z_3)\\
        Z_3 &= (\varphi_{K_3} \circ \cdots \circ \varphi_1)(Z_1, Z_2, Z_3)\\
    \end{split}
    \label{eq:bottleneck}
\end{equation}
As before the feature maps generated over the network is represented by $F_k =
\varphi_k(F_{k-1}) = \nu_k(W_k \otimes F_{k-1} + B_k) \in \mathbb{R}^{R_k \times
C_k \times D_k}$

The head of the network consists of a set of convolutional and reshape layers
that will transform the output of the bottleneck in a matrix $\hat{A}$ of
predictions which are encoded \emph{anchor boxes}, for simplicity we will denote
it as a simple function $\beta$ where $\hat{A} = \beta(Z_1, Z_2, Z_3) \in
\mathbb{R}^{\hat{M} \times (4 + |\{l_1, l_2\}|}$ and $\hat{M} = 3 \sum_{i=1}^3
R_{L_i} C_{L_i}$.

\subsection{Prediction decoding}
To get the bounding boxes $B_n$, is necessary to transform the output $\hat{A}$
knowing that each row $\hat{a}$ is represented by:
$$
\hat{a} = \left(\frac{x_b - x_a}{w_a}, \frac{y_b - y_a}{h_a},
\log{\frac{w_b}{w_a}}, \log{\frac{h_b}{h_a}}, p(l_1), p(l_2)\right)
$$ 
where $(x_a, y_a)$ and $(w_a, h_a)$ are the positions and dimensions of one
anchor box and $x_b, y_b$ are the positions of the associated bounding box with
dimensions $w_b, h_b$, thus the respective bounding box $\hat{b}$ associated with
$\hat{a}$ is:
$$
\hat{b} = \left(w_a \hat{a}_1 + x_a, h_a \hat{a}_2 + y_a,
e^{\hat{a}_3} w_a , e^{\hat{a}_4} h_a, l_i \right)
$$
where $i = \arg \max{(p(l_1), p(l_2))}$. Therefore the matrix $\hat{A}$ can be
decoded to get an associated bounding box matrix $\hat{B} \in \hat{M} \times 5$.

To get the final detection is necessary to use \emph{Non Maximum Suppression} to
filter overlapped detections.

\subsection{Evaluation Metrics}
To compare box detections against ground truth boxes is necessary to use the
\emph{Intersection over Union $IoU$} \eqref{eq:IoU}, a metric that measures the
overlap between to boxes, for example if a box perfectly match with another it's
$IoU$ will be 1 on the other hand when there's not overlap the metric will
output 0, the figure \ref{fig:IoU} illustrates it's operation.
\begin{equation}
    IoU = \frac{b_1 \cap b_2}{b_1 \cup b_2}
    \label{eq:IoU}
\end{equation}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{./figures/theory/IoU.png}
    \caption{Intersection Over Union}
    \label{fig:IoU}
\end{figure}
With this in mind is possible to determine if a bounding box is a \emph{True
Positive (TP)}, \emph{False Positive (FP)} or \emph{False Negative} (FN)
detection setting a threshold for the $IoU$.

In object detection challenges \cite{datasets:coco, datasets:pascal}, measures
like \emph{PR-curve, Average Precision (AP)} and \emph{mean Average Precision
(AP)} are commonly used. \emph{PR-curve} is obtained calculating the Precision
\eqref{eq:precision} and Recall \eqref{eq:recall} using different $IoU$
thresholds, the plotted values will show a monotonically decreasing function
(Figure \ref{fig:PR-curve}) which shows the precision recall trade-off.
\begin{equation}
    Precision = \frac{TP}{TP + FP}
    \label{eq:precision}
\end{equation}
\begin{equation}
    Recall = \frac{TP}{TP + FP}
    \label{eq:recall}
\end{equation}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/theory/PR-curve.png}
    \caption{PR-curve example for on class}
    \label{fig:PR-curve}
\end{figure}
The $AP$ is the Average of all calculated precisions for each class and finally
$mAP$ is the AP mean over the classes.

\section{Experimental set-up}
\printbibliography
\end{document}
